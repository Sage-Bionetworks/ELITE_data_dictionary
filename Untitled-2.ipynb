{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name: term_page_manager.py\n",
    "definition: a script to generate and delete annotation term page\n",
    "Contributors: Dan Lu\n",
    "\"\"\"\n",
    "# load modules\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "import string\n",
    "from functools import partial\n",
    "\n",
    "import frontmatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mdutils import fileutils\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"./_config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def get_term_info(data_model, term):\n",
    "    \"\"\"\n",
    "    Function to get a dictionary for term definition, definition source, module\n",
    "\n",
    "    :param term: the term name\n",
    "\n",
    "    :returns: a dictionary with keys: Description and Module\n",
    "    \"\"\"\n",
    "    # get the definition and module of the term from data model\n",
    "    results = data_model.loc[\n",
    "        data_model[\"Attribute\"] == term, [\"Description\", \"Source\", \"Module\"]\n",
    "    ].to_dict(\"records\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_page(data_model, term):\n",
    "    \"\"\"\n",
    "    Function to generate term/template markdown page\n",
    "\n",
    "    :param term: the term name\n",
    "\n",
    "    :returns: a term Markdown page generated under the docs/<module_name> folder\n",
    "    \"\"\"\n",
    "    term_file = re.sub(\" \", \"_\", term)\n",
    "    # get term information\n",
    "    results = get_term_info(data_model, term_attr)\n",
    "\n",
    "    # add paragraph for term definition and source\n",
    "    try:\n",
    "        if results[0][\"Source\"] == \"Sage Bionetworks\":\n",
    "            results[0][\"Source\"] = \"https://sagebionetworks.org/\"\n",
    "    except IndexError:\n",
    "        results[0][\"Source\"] = \"\"\n",
    "\n",
    "    if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "        # load template\n",
    "        post = frontmatter.load(\"template_page_template.md\")\n",
    "        post.metadata[\"title\"] = re.sub(\"([A-Z]+)\", r\" \\1\", term).title()\n",
    "        post.metadata[\"permalink\"] = f'docs/{post.metadata[\"title\"]}.html'\n",
    "    else:\n",
    "        # load template\n",
    "        post = frontmatter.load(\"term_page_template.md\")\n",
    "        post.metadata[\"title\"] = term\n",
    "\n",
    "    post.metadata[\"parent\"] = results[0][\"Module\"]\n",
    "\n",
    "    # load input data and term/template description\n",
    "    if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "        post.content = (\n",
    "            \"{% assign mydata=site.data.\"\n",
    "            + term_file\n",
    "            + \" %} \\n{: .note-title } \\n\"\n",
    "            + f\">{post.metadata['title']}\\n\"\n",
    "            + \">\\n\"\n",
    "            + f\">{results[0]['Description']} [[Source]]({results[0]['Source']})\\n\"\n",
    "            + post.content\n",
    "        )\n",
    "    else:\n",
    "        post.content = (\n",
    "            \"{% assign mydata=site.data.\"\n",
    "            + term_file\n",
    "            + \" %} \\n{: .note-title } \\n\"\n",
    "            + f\">{term}\\n\"\n",
    "            + \">\\n\"\n",
    "            + f\">{results[0]['Description']} [[Source]]({results[0]['Source']})\\n\"\n",
    "            + post.content\n",
    "        )\n",
    "\n",
    "    # create directory for the moduel if not exist\n",
    "    if not os.path.exists(f\"docs/{results[0]['Module']}/\"):\n",
    "        os.mkdir(f\"docs/{results[0]['Module']}/\")\n",
    "        # create a module page\n",
    "        module = fileutils.MarkDownFile(\n",
    "            f\"docs/{results[0]['Module']}/{results[0]['Module']}\"\n",
    "        )\n",
    "        if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "            # add permalink for template page\n",
    "            module.append_end(\n",
    "                f\"--- \\nlayout: page \\ntitle: {results[0]['Module']} \\nhas_children: true \\nnav_order: 5 \\npermalink: docs/{results[0]['Module']}.html \\n---\"\n",
    "            )\n",
    "        else:\n",
    "            module.append_end(\n",
    "                f\"--- \\nlayout: page \\ntitle: {results[0]['Module']} \\nhas_children: true \\nnav_order: 2 \\npermalink: docs/{results[0]['Module']}.html \\n---\"\n",
    "            )\n",
    "\n",
    "    # create file\n",
    "    file = fileutils.MarkDownFile(f\"docs/{results[0]['Module']}/{term}\")\n",
    "    # add content to the file\n",
    "    file.append_end(frontmatter.dumps(post))\n",
    "\n",
    "\n",
    "def delete_page(term):\n",
    "    for file in glob.glob(\"docs/*/*.md\"):\n",
    "        if file.split(\"/\")[-1].split(\".\")[0] == term:\n",
    "            os.remove(file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # load data model csv file\n",
    "    data_model = pd.read_csv(config[\"data_model\"])\n",
    "\n",
    "    # pull terms\n",
    "    term_files = [file.split(\"/\")[-1].split(\".\")[0]\n",
    "                  for file in glob.glob(\"_data/*.csv\")]\n",
    "\n",
    "    term_files_attr = [re.sub(\"_\", \" \", t) for t in term_files]\n",
    "\n",
    "    term_pages = [file.split(\"/\")[-1].split(\".\")[0]\n",
    "                  for file in glob.glob(\"docs/*/*.md\")]\n",
    "\n",
    "    term_pages_attr = [re.sub(\"_\", \" \", t) for t in term_pages]\n",
    "\n",
    "    to_add = map(str, np.setdiff1d(term_files_attr, term_pages_attr))\n",
    "\n",
    "    to_delete = np.setdiff1d(term_pages_attr, term_files_attr).tolist()\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    # generate pages for terms with the term files\n",
    "\n",
    "    generate_page_temp = partial(generate_page, data_model)\n",
    "\n",
    "    list(map(generate_page_temp, to_add))\n",
    "\n",
    "    # delete pages for terms without the term files and exclude module and template pages (since template page might be named differently from the template files)\n",
    "    to_delete = [\n",
    "        x\n",
    "        for x in to_delete\n",
    "        if x not in data_model[\"Module\"].dropna().unique().tolist() and \"Template\" not in x\n",
    "    ]\n",
    "\n",
    "    list(map(delete_page, to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.read_csv(config[\"data_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"Biospecimen human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_term_info(data_model, term):\n",
    "\"\"\"\n",
    "# # Function to get a dictionary for term definition, definition source, module\n",
    "\n",
    ":param term: the term name\n",
    "\n",
    ":returns: a dictionary with keys: Description and Module\n",
    "\"\"\"\n",
    "\n",
    "# get the definition and module of the term from data model\n",
    "results = data_model.loc[\n",
    "    data_model[\"Attribute\"] == term, [\"Description\", \"Source\", \"Module\"]\n",
    "].to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module frontmatter:\n",
      "\n",
      "load(fd, encoding='utf-8', handler=None, **defaults)\n",
      "    Load and parse a file-like object or filename,\n",
      "    return a :py:class:`post <frontmatter.Post>`.\n",
      "    \n",
      "    .. doctest::\n",
      "    \n",
      "        >>> post = frontmatter.load('tests/yaml/hello-world.txt')\n",
      "        >>> with open('tests/yaml/hello-world.txt') as f:\n",
      "        ...     post = frontmatter.load(f)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(frontmatter.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biospecimen_human'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def generate_page(data_model, term):\n",
    "    \"\"\"\n",
    "    Function to generate term/template markdown page\n",
    "\n",
    "    :param term: the term name\n",
    "\n",
    "    :returns: a term Markdown page generated under the docs/<module_name> folder\n",
    "    \"\"\"\n",
    "    term_file = re.sub(\" \", \"_\", term)\n",
    "    \n",
    "    # get term information\n",
    "    results = get_term_info(data_model, term_attr)\n",
    "\n",
    "    # add paragraph for term definition and source\n",
    "    try:\n",
    "        if results[0][\"Source\"] == \"Sage Bionetworks\":\n",
    "            results[0][\"Source\"] = \"https://sagebionetworks.org/\"\n",
    "    except IndexError:\n",
    "        results[0][\"Source\"] = \"\"\n",
    "\n",
    "    if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "        # load template\n",
    "        post = frontmatter.load(\"template_page_template.md\")\n",
    "        post.metadata[\"title\"] = re.sub(\"([A-Z]+)\", r\" \\1\", term).title()\n",
    "        post.metadata[\"permalink\"] = f'docs/{post.metadata[\"title\"]}.html'\n",
    "    else:\n",
    "        # load template\n",
    "        post = frontmatter.load(\"term_page_template.md\")\n",
    "        post.metadata[\"title\"] = term\n",
    "\n",
    "    post.metadata[\"parent\"] = results[0][\"Module\"]\n",
    "\n",
    "    # load input data and term/template description\n",
    "    if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "        post.content = (\n",
    "            \"{% assign mydata=site.data.\"\n",
    "            + term_file\n",
    "            + \" %} \\n{: .note-title } \\n\"\n",
    "            + f\">{post.metadata['title']}\\n\"\n",
    "            + \">\\n\"\n",
    "            + f\">{results[0]['Description']} [[Source]]({results[0]['Source']})\\n\"\n",
    "            + post.content\n",
    "        )\n",
    "    else:\n",
    "        post.content = (\n",
    "            \"{% assign mydata=site.data.\"\n",
    "            + term_file\n",
    "            + \" %} \\n{: .note-title } \\n\"\n",
    "            + f\">{term}\\n\"\n",
    "            + \">\\n\"\n",
    "            + f\">{results[0]['Description']} [[Source]]({results[0]['Source']})\\n\"\n",
    "            + post.content\n",
    "        )\n",
    "\n",
    "    # create directory for the moduel if not exist\n",
    "    if not os.path.exists(f\"docs/{results[0]['Module']}/\"):\n",
    "        os.mkdir(f\"docs/{results[0]['Module']}/\")\n",
    "        # create a module page\n",
    "        module = fileutils.MarkDownFile(\n",
    "            f\"docs/{results[0]['Module']}/{results[0]['Module']}\"\n",
    "        )\n",
    "        if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "            # add permalink for template page\n",
    "            module.append_end(\n",
    "                f\"--- \\nlayout: page \\ntitle: {results[0]['Module']} \\nhas_children: true \\nnav_order: 5 \\npermalink: docs/{results[0]['Module']}.html \\n---\"\n",
    "            )\n",
    "        else:\n",
    "            module.append_end(\n",
    "                f\"--- \\nlayout: page \\ntitle: {results[0]['Module']} \\nhas_children: true \\nnav_order: 2 \\npermalink: docs/{results[0]['Module']}.html \\n---\"\n",
    "            )\n",
    "\n",
    "    # create file\n",
    "    file = fileutils.MarkDownFile(f\"docs/{results[0]['Module']}/{term}\")\n",
    "    # add content to the file\n",
    "    file.append_end(frontmatter.dumps(post))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def delete_page(term):\n",
    "    for file in glob.glob(\"docs/*/*.md\"):\n",
    "        if file.split(\"/\")[-1].split(\".\")[0] == term:\n",
    "            os.remove(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def main():\n",
    "    # load data model csv file\n",
    "    data_model = pd.read_csv(config[\"data_model\"])\n",
    "\n",
    "    # pull terms\n",
    "    term_files = [file.split(\"/\")[-1].split(\".\")[0] for file in glob.glob(\"_data/*.csv\")]\n",
    "\n",
    "    term_files_attr = [re.sub(\"_\", \" \", t) for t in term_files]\n",
    "\n",
    "    term_pages = [file.split(\"/\")[-1].split(\".\")[0] for file in glob.glob(\"docs/*/*.md\")]\n",
    "\n",
    "    term_pages_attr = [re.sub(\"_\", \" \", t) for t in term_pages]\n",
    "\n",
    "    to_add = map(str, np.setdiff1d(term_files_attr, term_pages_attr))\n",
    "\n",
    "    to_delete = np.setdiff1d(term_pages_attr, term_files_attr).tolist()\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    # generate pages for terms with the term files\n",
    "\n",
    "    generate_page_temp = partial(generate_page, data_model)\n",
    "\n",
    "    list(map(generate_page_temp, to_add))\n",
    "\n",
    "    # delete pages for terms without the term files and exclude module and template pages (since template page might be named differently from the template files)\n",
    "    to_delete = [\n",
    "        x\n",
    "        for x in to_delete\n",
    "        if x not in data_model[\"Module\"].dropna().unique().tolist() and \"Template\" not in x\n",
    "    ]\n",
    "\n",
    "    list(map(delete_page, to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Biospecimen_nonHuman',\n",
       " 'ID_mapping',\n",
       " 'Individual_Human',\n",
       " 'Individual_nonHuman',\n",
       " 'Metabolomics_Human',\n",
       " 'Whole_Genome_Sequencing',\n",
       " 'analytical_covariates',\n",
       " 'bsSeq_(bisulfite-seq_WGBS_methylseq_methylomics)',\n",
       " 'data_dictionary',\n",
       " 'ethnicity',\n",
       " 'race']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Biospecimen human',\n",
       " 'Biospecimen nonHuman',\n",
       " 'ID mapping',\n",
       " 'Individual Human',\n",
       " 'Individual nonHuman',\n",
       " 'Metabolomics Human',\n",
       " 'Microbiome',\n",
       " 'RNAseq',\n",
       " 'Whole Genome Sequencing',\n",
       " 'acquisitionBatchID',\n",
       " 'acquisitionBatchSize',\n",
       " 'acquisitionBatchSizeUnit',\n",
       " 'age',\n",
       " 'analysisType',\n",
       " 'analytical covariates',\n",
       " 'assay',\n",
       " 'batchID',\n",
       " 'batchLabel',\n",
       " 'biospecimen',\n",
       " 'bsSeq (bisulfite-seq WGBS methylseq methylomics)',\n",
       " 'captivityDuration',\n",
       " 'captivityStatus',\n",
       " 'cohort',\n",
       " 'commonName',\n",
       " 'consentGroupID',\n",
       " 'consortium',\n",
       " 'countryCode',\n",
       " 'data dictionary',\n",
       " 'dataFile',\n",
       " 'dataSubtype',\n",
       " 'dataType',\n",
       " 'diagnosis',\n",
       " 'diagnosisStatus',\n",
       " 'directionalBSseqLibrary',\n",
       " 'dnaBatchID',\n",
       " 'dnaBatchSize',\n",
       " 'dnaBatchSizeUnit',\n",
       " 'ethnicGroupCode',\n",
       " 'ethnicity',\n",
       " 'fieldCenterCode',\n",
       " 'fileFormat',\n",
       " 'genotyping',\n",
       " 'grant',\n",
       " 'individual',\n",
       " 'individualID',\n",
       " 'isModelSystem',\n",
       " 'isMultiSpecimen',\n",
       " 'isStranded',\n",
       " 'libraryBatchID',\n",
       " 'libraryPrep',\n",
       " 'libraryPreparationMethod',\n",
       " 'libraryType',\n",
       " 'libraryVersion',\n",
       " 'lifeStage',\n",
       " 'manifest',\n",
       " 'metadata',\n",
       " 'metadataType',\n",
       " 'modelSystemName',\n",
       " 'modelSystemType',\n",
       " 'organ',\n",
       " 'platform',\n",
       " 'platformLocation',\n",
       " 'processingBatchID',\n",
       " 'processingBatchSize',\n",
       " 'processingBatchSizeUnit',\n",
       " 'proteomics',\n",
       " 'protocol',\n",
       " 'race',\n",
       " 'readLength',\n",
       " 'readLengthUnits',\n",
       " 'readStrandOrigin',\n",
       " 'reagentCatalogNumber',\n",
       " 'reagentContact',\n",
       " 'reagentID(s)',\n",
       " 'reagentLotNumber',\n",
       " 'reagentManufacturer',\n",
       " 'reagentName',\n",
       " 'reagentWeblink',\n",
       " 'referenceTranscriptID',\n",
       " 'resourceType',\n",
       " 'rnaBatchID',\n",
       " 'rnaBatchSize',\n",
       " 'rnaBatchSizeUnit',\n",
       " 'runType',\n",
       " 'sampleBatchID',\n",
       " 'sampleBatchSize',\n",
       " 'sampleBatchSizeUnit',\n",
       " 'samplingAge',\n",
       " 'scRNAseq',\n",
       " 'sequencingBatchID',\n",
       " 'sequencingBatchSize',\n",
       " 'sequencingBatchSizeUnit',\n",
       " 'sex',\n",
       " 'speciesAge',\n",
       " 'speciesGroup',\n",
       " 'speciesName',\n",
       " 'specimenAge',\n",
       " 'specimenID',\n",
       " 'studyCode',\n",
       " 'taxon',\n",
       " 'technologyPlatformVersion',\n",
       " 'tissue',\n",
       " 'totalReads',\n",
       " 'visitCode']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([re.sub(\"_\", \" \", t) for t in term_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"dataType\"\n",
    "\n",
    "\"\"\"_summary_\n",
    "\n",
    "Args:\n",
    "    data_model (_type_): _description_\n",
    "    term (_type_): _description_\n",
    "\"\"\"\n",
    "term_csv_name = re.sub(\"\\s|/\", \"_\", term)\n",
    "\n",
    "if \"Template\" in data_model.query(\"Attribute == @term\")[\"Module\"].values:\n",
    "    depends_on = get_template_keys(data_model, term)\n",
    "    new = data_model.loc[data_model[\"Attribute\"].isin(depends_on),]\n",
    "    new = new[\n",
    "        [\n",
    "            \"Attribute\",\n",
    "            \"Description\",\n",
    "            \"Type\",\n",
    "            \"Valid Values\",\n",
    "            \"DependsOn\",\n",
    "            \"Required\",\n",
    "            \"Source\",\n",
    "            \"Module\",\n",
    "        ]\n",
    "    ].reset_index(drop=True)\n",
    "    new.rename(\n",
    "        columns={\"Attribute\": \"Key\", \"Description\": \"Key Description\"}, inplace=True\n",
    "    )\n",
    "    # update template file\n",
    "    new.to_csv(os.path.join(\"./_data\", term_csv_name + \".csv\"), index=False)\n",
    "    print(\"\\033[92m {} \\033[00m\".format(f\"Updated {term}.csv\"))\n",
    "else:\n",
    "    # convert dataframe to long format\n",
    "    new = data_model.loc[data_model[\"Attribute\"] == term,][\n",
    "        [\"Attribute\", \"Valid Values\", \"DependsOn\", \"Type\", \"Module\"]\n",
    "    ]\n",
    "    new = (\n",
    "        new.drop(columns=[\"Attribute\", \"DependsOn\"])\n",
    "        .set_index([\"Type\", \"Module\"])\n",
    "        .apply(lambda x: x.str.split(\",\").explode())\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # add columns\n",
    "    new.rename(columns={\"Valid Values\": \"Key\"}, inplace=True)\n",
    "\n",
    "    new[\"Key\"] = new[\"Key\"].str.strip()\n",
    "\n",
    "    # load existing csv\n",
    "    old = pd.read_csv(f\"./_data/{term_csv_name}.csv\")\n",
    "    # upload existing csv if Key, Type or Module column is changed\n",
    "    if not (\n",
    "        new[\"Key\"].equals(old[\"Key\"])\n",
    "        and new[\"Type\"].equals(old[\"Type\"])\n",
    "        and new[\"Module\"].equals(old[\"Module\"])\n",
    "    ):\n",
    "        updated = new.astype(str).merge(\n",
    "            old.astype(str), how=\"left\", on=[\"Key\", \"Type\", \"Module\"]\n",
    "        )\n",
    "        updated[\"Type\"] = new[\"Type\"]\n",
    "        updated[\"Module\"] = new[\"Module\"]\n",
    "        updated = updated[[\"Key\", \"Key Description\",\n",
    "                           \"Type\", \"Source\", \"Module\"]]\n",
    "        updated.to_csv(\n",
    "            os.path.join(\"./_data\", term_csv_name + \".csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        print(\"\\033[92m {} \\033[00m\".format(f\"Updated {term_csv_name}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"Key\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"grant\"\n",
    "data_model.query(\"Attribute == @term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"AU/ml\"\n",
    "re.sub(\"\\s|/\", \"_\", term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"./_data\", term.replace(\"\\\\s\", \"_\") + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    term = ['Biospecimen_human', 'assay']\n",
    "    \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        term (_type_, optional): _description_. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    # load data model\n",
    "    data_model = pd.read_csv(config[\"data_model\"])\n",
    "\n",
    "    data_model['Attribute'] = data_model['Attribute'].str.replace(\"\\\\s|/\", \"_\", regex = True)\n",
    "\n",
    "    # get the list of existing term csvs\n",
    "    files = [\n",
    "        file.split(\".csv\")[0] for file in os.listdir(\"_data/\") if file.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if term:\n",
    "        df = data_model.loc[\n",
    "            (data_model[\"Module\"].notnull())\n",
    "            & (\n",
    "                data_model[\"Attribute\"].isin(term)\n",
    "                & (data_model[\"Parent\"] != \"validValue\")\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        df = data_model.loc[data_model[\"Module\"].notnull(),]\n",
    "\n",
    "    # generate files when term files don't exist. Do not add files for valid values or specify because these have no useful sub values or depends on\n",
    "    new_terms = df.loc[\n",
    "        (~df[\"Attribute\"].isin(files))\n",
    "        & (df[\"Parent\"] != \"validValue\")\n",
    "        & (~df[\"Attribute\"].str.contains(\"specify\")),\n",
    "        \"Attribute\",\n",
    "    ].tolist()\n",
    "\n",
    "    # generate csv by calling reformatter for each row of the df\n",
    "    generate_csv_temp = partial(generate_csv, data_model)\n",
    "\n",
    "    list(map(generate_csv_temp, new_terms))\n",
    "\n",
    "    # update files if the term files exist\n",
    "    exist_terms = df.loc[df[\"Attribute\"].isin(files), \"Attribute\"].tolist()\n",
    "\n",
    "    update_csv_temp = partial(update_csv, data_model)\n",
    "\n",
    "    list(map(update_csv_temp, exist_terms))\n",
    "\n",
    "    # delete term csv if the attribute is removed from data model\n",
    "    for file in files:\n",
    "        if file not in data_model.Attribute.values:\n",
    "            os.remove(f\"_data/{file}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.query('Parent == \"dataProperty\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter specification attributes\n",
    "data_model.loc[data_model[\"Attribute\"].str.contains(\n",
    "    \"specify\"), \"Parent\"] = \"Other\"\n",
    "data_model.loc[data_model[\"Attribute\"].str.contains(\n",
    "    \"specify\"), \"Module\"] = \"Other\"\n",
    "data_model.loc[\n",
    "    data_model[\"Attribute\"].str.contains(\"specify\"), \"Description\"\n",
    "] = \"Value is determined by the data contributor\"\n",
    "data_model.loc[data_model[\"Attribute\"].str.contains(\n",
    "    \"specify\"), \"Type\"] = \"String\"\n",
    "data_model.loc[\n",
    "    data_model[\"Attribute\"].str.contains(\"specify\"), \"Source\"\n",
    "] = \"Sage Bionetworks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.to_csv(\"EL.data.model.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-models-elite-98ShFYwe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
